{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "341e16d4-7a07-4a5a-b0f4-6d1e7d55b600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              headline  \\\n",
      "0    Super Micro Computer Announces 10-for-1 Stock ...   \n",
      "1          3 Dividend Stocks to Double Up on Right Now   \n",
      "2    S&P 500 Sell-Off: Is It Really Safe to Invest ...   \n",
      "3      Why Meta Platforms Stock Was Gaining on Tuesday   \n",
      "4    I Used to Think I'd Claim Social Security at 7...   \n",
      "..                                                 ...   \n",
      "105  Trump’s Overtures to US Crypto Sector Put Glob...   \n",
      "106      Prediction: Bitcoin Will Hit $100,000 by 2025   \n",
      "107                        The 'Trump trade' is fading   \n",
      "108  Stock market news today: Stocks rally to snap ...   \n",
      "109  Bitcoin bulls predict new record highs followi...   \n",
      "\n",
      "                                               alltext                 date  \\\n",
      "0    Recent advancements in the field of artificial...  2024-08-06 22:13:00   \n",
      "1    With signs pointing to the Federal Reserve bei...  2024-08-06 11:45:00   \n",
      "2    After enjoying record-breaking highs most of t...  2024-08-06 09:30:00   \n",
      "3    Shares of Meta Platforms (META 3.86%) moved hi...  2024-08-07 03:57:00   \n",
      "4    Social Security is a surprisingly flexible pro...  2024-08-07 03:54:00   \n",
      "..                                                 ...                  ...   \n",
      "105  (Bloomberg) -- Aspiring crypto hubs including ...  2024-08-07 06:00:00   \n",
      "106  Despite being a very volatile asset, it's impo...  2024-08-07 03:17:00   \n",
      "107  The whipsaw 2024 presidential election continu...  2024-08-07 00:49:00   \n",
      "108  US stocks finished Tuesday's session solidly i...  2024-08-06 23:55:00   \n",
      "109  Bitcoin (BTC-USD) backers are doubling down on...  2024-08-06 12:53:00   \n",
      "\n",
      "            source                                                url  \n",
      "0             FOOL  https://www.fool.com/investing/2024/08/06/supe...  \n",
      "1             FOOL  https://www.fool.com/investing/2024/08/06/3-di...  \n",
      "2             FOOL  https://www.fool.com/investing/2024/08/06/sp-5...  \n",
      "3             FOOL  https://www.fool.com/investing/2024/08/07/why-...  \n",
      "4             FOOL  https://www.fool.com/retirement/2024/08/07/i-u...  \n",
      "..             ...                                                ...  \n",
      "105  FINANCE-YAHOO  https://finance.yahoo.com/news/trump-overtures...  \n",
      "106  FINANCE-YAHOO  https://finance.yahoo.com/news/prediction-bitc...  \n",
      "107  FINANCE-YAHOO  https://finance.yahoo.com/news/the-trump-trade...  \n",
      "108  FINANCE-YAHOO  https://finance.yahoo.com/news/stock-market-ne...  \n",
      "109  FINANCE-YAHOO  https://finance.yahoo.com/news/bitcoin-bulls-p...  \n",
      "\n",
      "[110 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import warnings\n",
    "from dateutil.parser import parse\n",
    "import datetime as dt\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "\n",
    "class DataScraper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        client = pymongo.MongoClient('127.0.0.1', 27017)\n",
    "        db = client[\"news\"]\n",
    "        newsCollection = db[\"news_collection\"]\n",
    "\n",
    "        self.proxies = {\n",
    "            \"https\" : \"fnyproxy.fnylocal.com:8080\",\n",
    "            \"http\" : \"fnyproxy.fnylocal.com:8080\"\n",
    "        }\n",
    "\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def soup_maker(self, url):\n",
    "        \n",
    "        res = requests.get(url, proxies = self.proxies,headers=self.headers, verify = False)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        return soup\n",
    "            \n",
    "        \n",
    "    ####### FOOL ####### //div/div/header/h1[contains(@class, \"text-3xl font-medium tracking-tight text-gray-1100 leading-relative-2 md:text-5xl\")]\n",
    "    def scrape_fool_news_links(self):\n",
    "        \n",
    "        url = \"https://www.fool.com/investing-news/\"\n",
    "        \n",
    "        soup = self.soup_maker(url)\n",
    "        \n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        links_element = dom.xpath('//div/div[contains(@class, \"flex py-12px text-gray-1100\")]//a[contains(@class, \"flex-shrink-0 w-1/3 mr-16px sm:w-auto\")]')\n",
    "\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "        for link_ele in links_element:\n",
    "            combine_whole_link = \"https://www.fool.com\" + link_ele.get('href')\n",
    "            \n",
    "            title, text, date = self.go_inside_fool_links(link = combine_whole_link)\n",
    "            \n",
    "            new_row = {\"headline\": title, \"alltext\": text, \"date\": date, \"source\": \"FOOL\", \"url\": combine_whole_link}\n",
    "            new_rows_df = new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "    \n",
    "    \n",
    "    def go_inside_fool_links(self, link):\n",
    "        # for link in links: \n",
    "        soup = self.soup_maker(link)\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        title_element = dom.xpath('//div/div/header/h1[contains(@class, \"text-3xl font-medium tracking-tight text-gray-1100 leading-relative-2 md:text-5xl\")]/text()')\n",
    "        if title_element != []:\n",
    "            title = title_element[0]\n",
    "        else:\n",
    "            title = title_element\n",
    "        # title = 'titlegelecek'\n",
    "        \n",
    "        text_element =dom.xpath('//div/div[contains(@class, \"max-w-full\")]/div[contains(@class, \"article-body\")]/p//text()')\n",
    "        text = ''\n",
    "        self.text_element = text_element\n",
    "        for paragraph in text_element:\n",
    "            text = text + paragraph\n",
    "        # text= 'textgelecek'\n",
    "\n",
    "        date_element = dom.xpath('//div/div[contains(@class, \"text-lg font-medium text-gray-800 mt-12px md:mt-24px mb-24px\")]/text()[2]')\n",
    "        date = date_element\n",
    "        try:\n",
    "            date = date[0]\n",
    "            date = date[3:-1]\n",
    "        except:\n",
    "            pass\n",
    "        if 'Updated' in date:\n",
    "            date = date.replace('Updated ', '')\n",
    "                # orjinal format 'Jul 9, 2024 at 4:51AM'\n",
    "        # date = pd.to_datetime(date, format='%b %d, %Y at %I:%M%p', errors='coerce')\n",
    "        \n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "        # date = '13'\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "            \n",
    "        \n",
    "    ######################\n",
    "        \n",
    "    ####### BENZINGA #######\n",
    "    def urls_benzinga(self):\n",
    "        \n",
    "        sections = [\"news\", \"markets\", \"trading-ideas\"]\n",
    "        urls = []\n",
    "        for section in sections:\n",
    "            urls.append(\"https://www.benzinga.com/\" + section)\n",
    "\n",
    "        return urls\n",
    "        \n",
    "    def scrape_benzinga_news_links(self):\n",
    "        \n",
    "        urls = self.urls_benzinga()\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "        for url in urls:\n",
    "            soup = self.soup_maker(url)\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            links_element = dom.xpath('//div/div[contains(@class, \"newsfeed-card text-black\")]//a[contains(@class, \"post-card-article-link post-card-image image-wrapper\")]')\n",
    "\n",
    "\n",
    "            before_new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "            for link_ele in links_element:\n",
    "                combine_whole_link = link_ele.get('href')\n",
    "                title, text, date = self.go_inside_benzinga_links(link = combine_whole_link)\n",
    "\n",
    "\n",
    "\n",
    "                new_row = {\"headline\": title, \"alltext\": text, \"date\": date, \"source\": \"BENZINGA\", \"url\": combine_whole_link}\n",
    "                before_new_rows_df = before_new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            new_rows_df = new_rows_df.append(before_new_rows_df, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "    \n",
    "    \n",
    "    def go_inside_benzinga_links(self, link):\n",
    "        # for link in links:\n",
    "        soup = self.soup_maker(url = link)\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        title_element = dom.xpath('//div/div/div[contains(@class, \"layout-container mx-auto\")]/div[contains(@class, \"flex flex-col md:flex-row items-start md:items-center gap-2 md:gap-8\")]/h1/text()')\n",
    "        if title_element != []:\n",
    "            title = title_element[0]\n",
    "        else:\n",
    "            title = title_element\n",
    "        # title = 'titlegelecekqqq'\n",
    "\n",
    "        text_element =dom.xpath('//div/div[contains(@itemprop, \"articleBody\")]/div/div//p//text()')\n",
    "        text = ''\n",
    "        self.text_element = text_element\n",
    "        for paragraph in text_element:\n",
    "            text = text + paragraph\n",
    "\n",
    "        # paragraphs = text_element.find_all('p') if text_element else []\n",
    "        # text = '\\n'.join(p.get_text() for p in paragraphs)\n",
    "        # text= 'textgelecekq1234'\n",
    "\n",
    "        date_element = dom.xpath('//div/div/div[contains(@class, \"article-date-wrap\")]/span[contains(@class, \"date\")]/text()')\n",
    "        date = date_element[0]\n",
    "                # orjinal hali: 'July 9, 2024 4:51AM'\n",
    "        # date = pd.to_datetime(date, format='%B %d, %Y %I:%M%p', errors='coerce')\n",
    "        \n",
    "        date = pd.to_datetime(date)\n",
    "        # date = '13'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ####### FINANCE-YAHOO #######\n",
    "    def urls_yahoo(self):\n",
    "        \n",
    "        sections = [\"latest-news\", \"stock-market-news/\", \"yahoo-finance-originals/\", \"economic-news/\", \"earnings/\", \"tech/\", \"crypto/\"]\n",
    "        urls = []\n",
    "        for section in sections:\n",
    "            urls.append(\"https://finance.yahoo.com/topic/\" + section)\n",
    "\n",
    "        return urls\n",
    "        \n",
    "    def scrape_yahoo_news_links(self):\n",
    "        \n",
    "        urls = self.urls_yahoo()\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "        for url in urls:\n",
    "            soup = self.soup_maker(url)\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            links_element = dom.xpath('//div/div//li[contains(@class, \"js-stream-content\")]/div[contains(@class, \"Py(14px)\")]//a')\n",
    "\n",
    "\n",
    "            before_new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "            for link_ele in links_element:\n",
    "                combine_whole_link = link_ele.get('href')\n",
    "                if combine_whole_link[0] == 'h':\n",
    "                    title, text, date = self.go_inside_yahoo_links(link = combine_whole_link)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "                new_row = {\"headline\": title, \"alltext\": text, \"date\": date, \"source\": \"FINANCE-YAHOO\", \"url\": combine_whole_link}\n",
    "                before_new_rows_df = before_new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            new_rows_df = new_rows_df.append(before_new_rows_df, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "    \n",
    "    \n",
    "    def go_inside_yahoo_links(self, link):\n",
    "        # for link in links:\n",
    "        soup = self.soup_maker(url = link)\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        title_element = dom.xpath('//div//div[contains(@class, \"caas-title-wrapper\")]//text()')\n",
    "        if title_element != []:\n",
    "            title = title_element[0]\n",
    "        else:\n",
    "            title = title_element\n",
    "        # title = 'titlegelecekqqq'\n",
    "\n",
    "        text_element =dom.xpath('//div/div[contains(@class, \"caas-body\")]//p//text()')\n",
    "        text = ''\n",
    "        for paragraph in text_element:\n",
    "            text = text + paragraph\n",
    "\n",
    "        # paragraphs = text_element.find_all('p') if text_element else []\n",
    "        # text = '\\n'.join(p.get_text() for p in paragraphs)\n",
    "        # text= 'textgelecekq1234'\n",
    "\n",
    "        date_element = dom.xpath('//div/div[contains(@class, \"caas-attr-time-style\")]//text()')\n",
    "        date = date_element[0]\n",
    "        if 'Updated' in date:\n",
    "            date_element = dom.xpath('//div/div[contains(@class, \"caas-attr-time-style\")]//time/text()')\n",
    "            date = date_element[0]\n",
    "            \n",
    "            #orjinal hali: 'Tue, Jul 9, 2024, 4:51AM'\n",
    "        # date = pd.to_datetime(date, format='%a, %b %d, %Y, %I:%M%p', errors='coerce')\n",
    "        \n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "        # date = date.split(', ', 1)[1].strip()\n",
    "        \n",
    "        # date = '13'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ####### BARRONS ####### need subscription !!!!!!!!!!!!\n",
    "    def urls_barrons(self):\n",
    "        \n",
    "        sections = [\"markets\", \"europe\", \"asia\", \"technology\", \"earnings/\", \"economy-and-policy\"]\n",
    "        urls = []\n",
    "        for section in sections:\n",
    "            urls.append(\"https://www.barrons.com/topics/\" + section)\n",
    "\n",
    "        return urls\n",
    "        \n",
    "    def scrape_barrons_news_links(self):\n",
    "        \n",
    "        urls = self.urls_barrons()\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "        for url in urls:\n",
    "            soup = self.soup_maker(url)\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            links_element = dom.xpath('//div/div[contains(@class, \"BarronsTheme--button-label--3EhEhI0I\")]/a')\n",
    "\n",
    "\n",
    "            before_new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "            for link_ele in links_element:\n",
    "                combine_whole_link = link_ele.get('href')\n",
    "                \n",
    "                title, text, date = self.go_inside_barrons_links(link = combine_whole_link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                new_row = {\"headline\": title, \"alltext\": \"text\", \"date\": \"date\", \"source\": \"BARRONS\", \"url\": combine_whole_link}\n",
    "                before_new_rows_df = before_new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            new_rows_df = new_rows_df.append(before_new_rows_df, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "    \n",
    "    \n",
    "    def go_inside_barrons_links(self, link):\n",
    "        # for link in links:\n",
    "        soup = self.soup_maker(url = link)\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        # title_element = dom.xpath('//div/div[contains(@class, \"standard__HeadlineWrapper-sc-14sjre0-12\")]//text()')\n",
    "        # title = title_element[0]\n",
    "\n",
    "        # title = 'titlegelecekqqq'\n",
    "        title = soup.prettify()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # text_element =dom.xpath('//div/section[contains(@class, \"css-yfonvn-Container ef4qpkp0\")]//p//text()')\n",
    "        # text = ''\n",
    "        # for paragraph in text_element:\n",
    "        #     text = text + paragraph\n",
    "\n",
    "        text= 'textgelecekq1234'\n",
    "\n",
    "        # date_element = dom.xpath('//div/div/p[contains(@class, \"css-1v6wp4h-TimeTag\")]//text()')\n",
    "        # date = date_element[0]\n",
    "        date = '13'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        ####### CNBC ####### yesterday's all news -------> tekrar bak\n",
    "\n",
    "    def scrape_cnbc_news_links(self):\n",
    "        \n",
    "        url = \"https://www.cnbc.com/site-map/articles/yesterday/\"\n",
    "        \n",
    "        soup = self.soup_maker(url)\n",
    "        \n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        links_element = dom.xpath('//div//div[contains(@class, \"SiteMapArticleList-articleData\")]/ul/li/a')\n",
    "\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "        for link_ele in links_element:\n",
    "            combine_whole_link = link_ele.get('href')\n",
    "            \n",
    "            # title, text, date = self.go_inside_cnbc_links(link = combine_whole_link)\n",
    "            \n",
    "            new_row = {\"headline\": \"title\", \"alltext\": \"text\", \"date\": \"date\", \"source\": \"CNBC\", \"url\": combine_whole_link}\n",
    "            new_rows_df = new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "    \n",
    "    \n",
    "    def go_inside_cnbc_links(self, link):\n",
    "        # for link in links: \n",
    "        soup = self.soup_maker(link)\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        title_element = dom.xpath('//div/div/header/h1[contains(@class, \"text-3xl font-medium tracking-tight text-gray-1100 leading-relative-2 md:text-5xl\")]/text()')\n",
    "        if title_element != []:\n",
    "            title = title_element[0]\n",
    "        else:\n",
    "            title = title_element\n",
    "        # title = 'titlegelecek'\n",
    "        \n",
    "        text_element =dom.xpath('//div/div[contains(@class, \"max-w-full\")]/div[contains(@class, \"article-body\")]/p//text()')\n",
    "        text = ''\n",
    "        self.text_element = text_element\n",
    "        for paragraph in text_element:\n",
    "            text = text + paragraph\n",
    "        # text= 'textgelecek'\n",
    "\n",
    "        date_element = dom.xpath('//div/div[contains(@class, \"text-lg font-medium text-gray-800 mt-12px md:mt-24px mb-24px\")]/text()[2]')\n",
    "        date = date_element\n",
    "        try:\n",
    "            date = date[0]\n",
    "            date = date[3:-1]\n",
    "        except:\n",
    "            pass\n",
    "        if 'Updated' in date:\n",
    "            date = date.replace('Updated ', '')\n",
    "                # orjinal format 'Jul 9, 2024 at 4:51AM'\n",
    "        # date = pd.to_datetime(date, format='%b %d, %Y at %I:%M%p', errors='coerce')\n",
    "        \n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "        # date = '13'\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def scrape_all_sites(self):\n",
    "        \n",
    "        df_data = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "        df_data = df_data.append(self.scrape_fool_news_links(), ignore_index=True)\n",
    "        df_data = df_data.append(self.scrape_benzinga_news_links(), ignore_index=True)\n",
    "        df_data = df_data.append(self.scrape_yahoo_news_links(), ignore_index=True)\n",
    "        # df_data = df_data.append(self.scrape_cnbc_news_links(), ignore_index=True)    # tekrar bakılmaı linkler çekilmiyor\n",
    "        # df_data = df_data.append(self.scrape_barrons_news_links(), ignore_index=True)  # need subscription !!!!!!!!!!!!  captcha\n",
    "        return df_data\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    scraper = DataScraper()\n",
    "    data = scraper.scrape_all_sites()\n",
    "    data.to_excel(\"last_news.xlsx\", index = False, engine = 'openpyxl')\n",
    "\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32915f34-b8fe-4e55-be50-83160b732079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "5b615c3c-7edd-4005-a8bb-c6da2aa5df9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2024-08-06\n",
      "1     2024-08-06\n",
      "2     2024-08-06\n",
      "3     2024-08-07\n",
      "4     2024-08-07\n",
      "         ...    \n",
      "61    2024-08-07\n",
      "62    2024-08-07\n",
      "63    2024-08-07\n",
      "64    2024-08-07\n",
      "65    2024-08-06\n",
      "Name: date, Length: 66, dtype: object\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import warnings\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# client = pymongo.MongoClient('10.128.96.25', 27017)\n",
    "# db = client[\"news\"]\n",
    "\n",
    "mydf33 = data\n",
    "\n",
    "newsCollection = mydf33\n",
    "\n",
    "newsCollection = newsCollection.reset_index(drop=True)\n",
    "## delete empty rows\n",
    "\n",
    "i = 0\n",
    "while i <= len(newsCollection)-1:\n",
    "    if newsCollection['headline'][i] == []:\n",
    "        newsCollection = newsCollection.drop(i)\n",
    "    i += 1\n",
    "        \n",
    "\n",
    "newsCollection = newsCollection.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sectors = [  'Diversified Telecommunication Services', 'Entertainment','Interactive Media & Services','Media',\n",
    "             'Wireless Telecommunication Services','Show Consumer Discretionary details','Consumer Discretionary','Auto Components',\n",
    "             'Automobiles','Distributors','Diversified Consumer Services','Hotels, Restaurants & Leisure','Household Durables',\n",
    "             'Internet & Direct Marketing Retail','Leisure Products','Multiline Retail','Specialty Retail','Textiles, Apparel & Luxury Goods',\n",
    "             'Consumer Staples','Beverages','Food & Staples Retailing','Food Products','Household Products','Personal Products','Tobacco',\n",
    "             'Show Energy details','Energy','2 Industries','Energy Equipment & Services','Oil, Gas & Consumable Fuels','Financials','Banks',\n",
    "             'Capital Markets','Consumer Finance','Diversified Financial Services','Insurance','Mortgage REITs','Thrifts & Mortgage Finance',\n",
    "             'Show Health Care details','Health Care','6 Industries','Biotechnology','Health Care Equipment & Supplies',\n",
    "             'Health Care Providers & Services','Health Care Technology','Life Sciences Tools & Services','Pharmaceuticals',\n",
    "             'Industrials','Aerospace & Defense','Air Freight & Logistics','Airlines','Building Products','Commercial Services & Supplies',\n",
    "             'Construction & Engineering','Electrical Equipment','Industrial Conglomerates','Machinery','Marine','Professional Services',\n",
    "             'Road & Rail','Trading Companies & Distributors','Transportation Infrastructure','Information Technology','Communications Equipment',\n",
    "             'Electronic Equipment, Instruments & Components','IT Services','Semiconductors','Semiconductor Equipment','Software',\n",
    "             'Technology Hardware, Storage & Peripherals','Materials','Chemicals','Construction Materials','Containers & Packaging',\n",
    "             'Metals & Mining','Paper & Forest Products','Real Estate','Equity Real Estate Investment Trusts',\n",
    "             'Real Estate Management & Development','Show Utilities details','Utilities','Electric Utilities',\n",
    "             'Gas Utilities','Independent Power and Renewable Electricity Producers','Multi-Utilities','Water Utilities']\n",
    "\n",
    "\n",
    "others = [   \"EMERGING MARKET\", \"GOLD\", \n",
    "             'Nasdaq', 'S&P 500', \"Dow Jones\", \"Russell 2000\", \"Brent Crude Oil\", \"Brent Petrol\", \"Gold Price\" \"Ons Gold\",\n",
    "             \"FTSE\", \"Nikkei\", \"Vix\", \"Dax\", \"Dax 40\", \"Euro Stoxx\", \"NYSE\", \"Silver\", \"Treasury Yields\", \n",
    "             \"COVID\", \"covid\",\"Trade War\",\"energy shortage\", \"Energy Prices\",\n",
    "             \"stagflasion\", \"Inflation\", \"recession\"]\n",
    "usTickers = ['3M', 'A. O. Smith','Abbott Laboratories','AbbVie','Accenture','Activision Blizzard',\n",
    "             'ADM (company)','Adobe Inc.','ADP (company)','Advance Auto Parts','AES Corporation','Aflac','Agilent Technologies',\n",
    "             'Air Products and Chemicals','Akamai','Alaska Air Group','Albemarle Corporation','Alexandria Real Estate Equities',\n",
    "             'Align Technology','Allegion','Alliant Energy','Allstate','Alphabet Inc.','Alphabet Inc.','Altria','Amazon','Amcor',\n",
    "             'Advanced Micro Devices','Ameren','American Airlines Group','American Electric Power','American Express','American International Group',\n",
    "             'American Tower','American Water Works','Ameriprise Financial','AmerisourceBergen','Ametek','Amgen','Amphenol',\n",
    "             'Analog Devices','Ansys','Aon (company)','APA Corporation','Apple Inc.','Applied Materials','Aptiv',\n",
    "             'Arch Capital Group','Arista Networks','Arthur J. Gallagher & Co.','Assurant','AT&T','Atmos Energy',\n",
    "             'Autodesk','AutoZone','AvalonBay Communities','Avery Dennison','Baker Hughes','Ball Corporation',\n",
    "             'Bank of America','Bath & Body Works, Inc.','Baxter International','BD (company)','W. R. Berkley Corporation',\n",
    "             'Berkshire Hathaway','Best Buy','Bio-Rad','Bio-Techne','Biogen','BlackRock','BNY Mellon','Boeing','Booking Holdings',\n",
    "             'BorgWarner','Boston Properties','Boston Scientific','Bristol Myers Squibb','Broadcom Inc.','Broadridge Financial Solutions',\n",
    "             'Brown & Brown','Brown–Forman','C.H. Robinson','Cadence Design Systems','Caesars Entertainment (2020)','Camden Property Trust',\n",
    "             'Campbell Soup Company','Capital One','Cardinal Health','CarMax','Carnival Corporation & plc','Carrier Global','Catalent',\n",
    "             'Caterpillar Inc.','Cboe Global Markets','CBRE Group','CDW','Celanese','Centene Corporation','CenterPoint Energy',\n",
    "             'Ceridian','CF Industries','Charles River Laboratories','Charles Schwab Corporation','Charter Communications',\n",
    "             'Chevron Corporation','Chipotle Mexican Grill','Chubb Limited','Church & Dwight','Cigna','Cincinnati Financial',\n",
    "             'Cintas','Cisco','Citigroup','Citizens Financial Group','Clorox','CME Group','CMS Energy','The Coca-Cola Company',\n",
    "             'Cognizant','Colgate-Palmolive','Comcast','Comerica','Conagra Brands','ConocoPhillips','Consolidated Edison',\n",
    "             'Constellation Brands','Constellation Energy','CooperCompanies','Copart','Corning Inc.','Corteva','CoStar Group',\n",
    "             'Costco','Coterra','Crown Castle','CSX Corporation','Cummins','CVS Health','D.R. Horton','Danaher Corporation',\n",
    "             'Darden Restaurants','DaVita Inc.','John Deere','Delta Air Lines','Dentsply Sirona','Devon Energy','Dexcom',\n",
    "             'Diamondback Energy','Digital Realty','Discover Financial','Dish Network','Disney','Dollar General',\n",
    "             'Dollar Tree','Dominion Energy',\"Domino's\",'Dover Corporation','Dow Inc.','DTE Energy','Duke Energy',\n",
    "             'DuPont','DXC Technology','Eastman Chemical Company','Eaton Corporation','EBay','Ecolab','Edison International',\n",
    "             'Edwards Lifesciences','Electronic Arts','Elevance Health','Eli Lilly and Company','Emerson Electric','Enphase',\n",
    "             'Entergy','EOG Resources','EPAM Systems','EQT','Equifax','Equinix','Equity Residential','Essex Property Trust',\n",
    "             'The Estée Lauder Companies','Etsy','Everest Re','Evergy','Eversource','Exelon','Expedia Group','Expeditors International',\n",
    "             'Extra Space Storage','ExxonMobil','F5, Inc.','FactSet','Fastenal','Federal Realty','FedEx','Fifth Third Bank',\n",
    "             'First Republic Bank','First Solar','FirstEnergy','FIS (company)','Fiserv','Fleetcor','FMC Corporation',\n",
    "             'Ford Motor Company','Fortinet','Fortive','Fox Corporation','Fox Corporation','Franklin Templeton',\n",
    "             'Freeport-McMoRan','Garmin','Gartner','Gen Digital','Generac','General Dynamics','General Electric',\n",
    "             'General Mills','General Motors','Genuine Parts Company','Gilead Sciences','Globe Life','Global Payments',\n",
    "             'Goldman Sachs','Halliburton','The Hartford','Hasbro','HCA Healthcare','Healthpeak Properties','Henry Schein',\n",
    "             'The Hershey Company','Hess Corporation','Hewlett Packard Enterprise','Hilton Worldwide','Hologic','The Home Depot',\n",
    "             'Honeywell','Hormel Foods','Host Hotels & Resorts','Howmet Aerospace','HP Inc.','Humana','Huntington Bancshares',\n",
    "             'Huntington Ingalls Industries','IBM','IDEX Corporation','Idexx Laboratories','Illinois Tool Works','Illumina, Inc.',\n",
    "             'Incyte','Ingersoll Rand','Intel','Intercontinental Exchange','International Paper','The Interpublic Group of Companies',\n",
    "             'International Flavors & Fragrances','Intuit','Intuitive Surgical','Invesco','Invitation Homes','IQVIA','Iron Mountain (company)',\n",
    "             'J.B. Hunt','Jack Henry & Associates','Jacobs Solutions','Johnson & Johnson','Johnson Controls','JPMorgan Chase','Juniper Networks',\n",
    "             \"Kellogg's\",'Keurig Dr Pepper','KeyBank','Keysight','Kimberly-Clark','Kimco Realty','Kinder Morgan','KLA Corporation',\n",
    "             'Kraft Heinz','Kroger','L3Harris','LabCorp','Lam Research','Lamb Weston','Las Vegas Sands','Leidos','Lennar',\n",
    "             'Lincoln Financial','Linde plc','Live Nation Entertainment','LKQ Corporation','Lockheed Martin','Loews Corporation',\n",
    "             \"Lowe's\",'Lumen Technologies','LyondellBasell','M&T Bank','Marathon Oil','Marathon Petroleum','MarketAxess',\n",
    "             'Marriott International','Marsh McLennan','Martin Marietta Materials','Masco','Mastercard','Match Group',\n",
    "             'McCormick & Company',\"McDonald's\",'McKesson','Medtronic','Merck & Co.','Meta Platforms','MetLife','Mettler Toledo',\n",
    "             'MGM Resorts','Microchip Technology','Micron Technology','Microsoft','Mid-America Apartment Communities','Moderna',\n",
    "             'Mohawk Industries','Molina Healthcare','Molson Coors Beverage Company','Mondelez International','Monolithic Power Systems',\n",
    "             'Monster Beverage',\"Moody's Corporation\",'Morgan Stanley','The Mosaic Company','Motorola Solutions','MSCI','Nasdaq, Inc.',\n",
    "             'NetApp','Netflix','Newell Brands','Newmont','News Corp','News Corp','NextEra Energy','Nike, Inc.','NiSource',\n",
    "             'Nordson Corporation','Norfolk Southern Railway','Northern Trust','Northrop Grumman','Norwegian Cruise Line Holdings',\n",
    "             'NRG Energy','Nucor','Nvidia','NVR, Inc.','NXP Semiconductors',\"O'Reilly Auto Parts\",'Occidental Petroleum',\n",
    "             'Old Dominion Freight Line','Omnicom Group','ON Semiconductor','Oneok','Oracle Corporation','Organon & Co.',\n",
    "             'Otis Worldwide','Paccar','Packaging Corporation of America','Paramount Global','Parker Hannifin','Paychex',\n",
    "             'Paycom','PayPal','Pentair','PepsiCo','PerkinElmer','Pfizer','PG&E','Philip Morris International','Phillips 66',\n",
    "             'Pinnacle West','Pioneer Natural Resources','PNC Financial Services','Pool Corporation (page does not exist)',\n",
    "             'PPG Industries','PPL Corporation','Principal Financial Group','Procter & Gamble','Progressive Corporation',\n",
    "             'Prologis','Prudential Financial','Public Service Enterprise Group','PTC (software company)','Public Storage',\n",
    "             'PulteGroup','Qorvo','Quanta Services','Qualcomm','Quest Diagnostics','Ralph Lauren Corporation','Raymond James',\n",
    "             'Raytheon Technologies','Realty Income','Regency Centers','Regeneron','Regions Financial Corporation','Republic Services',\n",
    "             'ResMed','Robert Half','Rockwell Automation','Rollins, Inc.','Roper Technologies','Ross Stores','Royal Caribbean Group',\n",
    "             'S&P Global','Salesforce','SBA Communications','Schlumberger','Seagate Technology','Sealed Air','Sempra Energy',\n",
    "             'ServiceNow','Sherwin-Williams','Signature Bank','Simon Property Group','Skyworks Solutions',\n",
    "             'The J.M. Smucker Company','Snap-on','SolarEdge','Southern Company','Southwest Airlines','Stanley Black & Decker',\n",
    "             'Starbucks','State Street Corporation','Steel Dynamics','Steris','Stryker Corporation','Silicon Valley Bank',\n",
    "             'Synchrony Financial','Synopsys','Sysco','T-Mobile US','T. Rowe Price','Take-Two Interactive','Tapestry, Inc.',\n",
    "             'Targa Resources','Target Corporation','TE Connectivity','Teledyne Technologies','Teleflex','Teradyne','Tesla, Inc.',\n",
    "             'Texas Instruments','Textron','Thermo Fisher Scientific','TJX Companies','Tractor Supply','Trane Technologies',\n",
    "             'TransDigm Group','The Travelers Companies','Trimble Inc.','Truist Financial','Tyler Technologies','Tyson Foods',\n",
    "             'U.S. Bank','UDR, Inc.','Ulta Beauty','Union Pacific Corporation','United Airlines Holdings','United Parcel Service',\n",
    "             'United Rentals','UnitedHealth Group','Universal Health Services','Valero Energy','Ventas (company)','Verisign',\n",
    "             'Verisk','Verizon','Vertex Pharmaceuticals','VF Corporation','Viatris','Vici Properties','Visa Inc.',\n",
    "             'Vornado Realty Trust','Vulcan Materials Company','Wabtec','Walgreens Boots Alliance','Walmart',\n",
    "             'Warner Bros. Discovery','Waste Management (corporation)','Waters Corporation','WEC Energy Group',\n",
    "             'Wells Fargo','Welltower','West Pharmaceutical Services','Western Digital','WestRock','Weyerhaeuser',\n",
    "             'Whirlpool Corporation','Williams Companies','Willis Towers Watson','W. W. Grainger','Wynn Resorts',\n",
    "             'Xcel Energy','Xylem Inc.','Yum! Brands','Zebra Technologies','Zimmer Biomet','Zions Bancorporation','Zoetis']\n",
    "usTickers = usTickers + sectors + others\n",
    "\n",
    "usTickers = [each.upper() for each in usTickers]\n",
    "\n",
    "torch.set_num_threads(6) \n",
    "def calculateSentiment(text):\n",
    "    inputs = tokenizer([text], padding = True, truncation = True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    model.config.id2label\n",
    "    #Tweet #Positive #Negative #Neutral\n",
    "    positive = predictions[:, 0].tolist()\n",
    "    negative = predictions[:, 1].tolist()\n",
    "    neutral = predictions[:, 2].tolist()\n",
    "    \n",
    "    table = {\n",
    "             \"Positive\":positive,\n",
    "             \"Negative\":negative, \n",
    "             \"Neutral\":neutral}\n",
    "    return positive[0], negative[0], neutral[0]\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s.upper() for s in sentences]\n",
    "    return sentences\n",
    "proxies = {\n",
    "    \"https\" : \"fnyproxy.fnylocal.com:8080\",\n",
    "    \"http\" : \"fnyproxy.fnylocal.com:8080\"\n",
    "}\n",
    "\n",
    "\n",
    "newsCollection.reset_index(inplace = True, drop = True)\n",
    "#a[\"date\"] = a[\"date\"].apply(lambda x:  x[:10] if type(x) != type(datetime.datetime(2022,11,11)) else x)\n",
    "#a[\"date\"] = a[\"date\"].apply(lambda x:  datetime.datetime.strptime(x, '%Y-%m-%d') if type(x) != type(datetime.datetime(2022,11,11)) else x)\n",
    "# a[\"date\"] = a[\"date\"].apply(lambda x: x.replace(hour = 0, minute = 0, second = 0, microsecond = 0))\n",
    "i = 0\n",
    "for d in newsCollection[\"date\"]:\n",
    "    newsCollection[\"date\"][i] = str(d)[:10]\n",
    "    i += 1\n",
    "print(newsCollection[\"date\"])\n",
    "\n",
    "#a = a[a[\"date\"] >= datetime.datetime(2023,8,19)]\n",
    "# a = a.reset_index().drop(\"index\", axis = 1)\n",
    "a = newsCollection\n",
    "\n",
    "array = np.array(a.headline)\n",
    "print(len(a))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", proxies = proxies)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", proxies = proxies)\n",
    "def historicalSentiment(df):\n",
    "    tempTickers = []\n",
    "    tempPosList = []\n",
    "    tempNegList = []\n",
    "    tempNeuList = []\n",
    "    sentence_count = 0\n",
    "    ticker_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sentenceList = split_into_sentences(row['alltext'])\n",
    "        for sentence in sentenceList:\n",
    "            sentence_count += 1 \n",
    "            for ticker in usTickers:\n",
    "                if ticker in sentence:\n",
    "                    ticker_count += 1 \n",
    "                    tempTickers.append(ticker)\n",
    "                    sentiment = calculateSentiment(sentence)\n",
    "                    tempPosList.append(sentiment[0])\n",
    "                    tempNegList.append(sentiment[1])\n",
    "                    tempNeuList.append(sentiment[2])\n",
    "    \n",
    "    if tempTickers:\n",
    "        resultDf = pd.DataFrame(list(zip(tempTickers, tempPosList, tempNegList, tempNeuList)), columns=[\"ticker\", \"positive\", \"negative\", \"neutral\"])\n",
    "        resultDf[\"positive\"] = resultDf[\"positive\"] * 100\n",
    "        resultDf[\"negative\"] = resultDf[\"negative\"] * 100\n",
    "        resultDf[\"neutral\"] = resultDf[\"neutral\"] * 100\n",
    "        resultDf[\"date\"] = df['date'].iloc[-1]\n",
    "        resultDf = resultDf.groupby([\"ticker\"]).mean().reset_index()\n",
    "    else:\n",
    "        resultDf = pd.DataFrame()\n",
    "\n",
    "    return resultDf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76e299-8b0d-49c4-a00a-6352b2878e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862f006-5a58-4be1-ba17-d128b08adb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure columns exist\n",
    "if 'tickers' not in a.columns:\n",
    "    a['tickers'] = np.nan\n",
    "if 'positive' not in a.columns:\n",
    "    a['positive'] = np.nan\n",
    "if 'negative' not in a.columns:\n",
    "    a['negative'] = np.nan\n",
    "if 'neutral' not in a.columns:\n",
    "    a['neutral'] = np.nan\n",
    "\n",
    "# Ensure each column is ready to accept list type\n",
    "a['tickers'] = a['tickers'].astype(object)\n",
    "a['positive'] = a['positive'].astype(object)\n",
    "a['negative'] = a['negative'].astype(object)\n",
    "a['neutral'] = a['neutral'].astype(object)\n",
    "\n",
    "# Define the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", proxies=proxies)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", proxies=proxies)\n",
    "\n",
    "# Update historicalSentiment function if needed\n",
    "\n",
    "# Iterate over DataFrame safely\n",
    "for index, row in a.iterrows():\n",
    "    # Single row DataFrame\n",
    "    single_news_df = a.loc[[index]]\n",
    "\n",
    "    # Analyze sentiment\n",
    "    sentiment_df = historicalSentiment(single_news_df)\n",
    "\n",
    "    # If sentiment_df is not empty, store results\n",
    "    if not sentiment_df.empty:\n",
    "        a.at[index, 'tickers'] = sentiment_df['ticker'].tolist()\n",
    "        a.at[index, 'positive'] = sentiment_df['positive'].tolist()\n",
    "        a.at[index, 'negative'] = sentiment_df['negative'].tolist()\n",
    "        a.at[index, 'neutral'] = sentiment_df['neutral'].tolist()\n",
    "    else:\n",
    "        a.at[index, 'tickers'] = []\n",
    "        a.at[index, 'positive'] = []\n",
    "        a.at[index, 'negative'] = []\n",
    "        a.at[index, 'neutral'] = []\n",
    "\n",
    "# Check results\n",
    "# print(a[['date', 'tickers', 'positive', 'negative', 'neutral']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4971d6-daf0-4da1-9975-fa993845a3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = a.drop_duplicates(subset = ['headline'])\n",
    "a['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb8e33-992d-4145-a944-d39f8a832be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a['status'] = np.nan\n",
    "a['status'] = a['status'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8583ef3-8bc1-4743-a4a1-f58a765a5e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13892a8-8a7c-4667-bd3d-560d2d038921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row_index in range(0,len(a)):\n",
    "    status = []\n",
    "    for ticker_index in range(0,len(a['tickers'][row_index])):\n",
    "        sentiment = {'positive' : a['positive'][row_index][ticker_index], 'negative' : a['negative'][row_index][ticker_index], 'neutral' : a['neutral'][row_index][ticker_index]}\n",
    "        compare = [(value, key) for key, value in sentiment.items()]\n",
    "        status.append(max(compare)[1])\n",
    "    a['status'][row_index] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bf9e1-a036-433f-bd31-aabbafa7845a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac94ccc-d01a-4e40-ba87-29ae4cfbda5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c882af7-fcf6-476f-b609-6b004b84b7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('10.128.96.25', 27017)\n",
    "db = client[\"sentiment_news\"]\n",
    "sentiment_newsCollection = db[\"sentiment_newsCollection\"]\n",
    "sentiment_news_df = pd.DataFrame(list(sentiment_newsCollection.find()))\n",
    "\n",
    "sentiment_news_df = sentiment_news_df.drop('_id', axis=1)\n",
    "db.sentiment_newsCollection.drop()\n",
    "\n",
    "b = pd.concat([sentiment_news_df, a])\n",
    "b = b.drop_duplicates(subset=['alltext', 'headline'], ignore_index=True)\n",
    "# print(sum(b.duplicated(subset=['headline'])))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc9e84-54f2-42e3-ae06-90233ef45fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pymongo\n",
    "\n",
    "Connection_String = \"mongodb://mongohost:27017\"\n",
    "session = pymongo.MongoClient('10.128.96.25', 27017)\n",
    "db = session['sentiment_news']\n",
    "collection = db['sentiment_newsCollection']\n",
    "\n",
    "data_records_b = b.to_dict(\"records\")\n",
    "collection.insert_many(data_records_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635d4f9-8636-4a08-a4d6-a80a863f2665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ec5f5-50d7-4899-8b99-053dfb35e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[b['date'] == '2024-08-05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfca18-4ee6-4667-82bf-3fe43b5f1d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
